{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd517100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import ast\n",
    "import nltk\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bf791c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, SelectKBest, f_classif, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d81b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "TEXT_COL_NAME = 'text_split'\n",
    "TEXT_TOKEN_COL_NAME = 'text_token'\n",
    "CLASS_COL_NAME = 'reduced_tags'\n",
    "\n",
    "DF_FILE_PATH = './data/no_stop.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6a91dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_SAVE_FILE_NAME = './Model/DT.bin'\n",
    "RF_SAVE_FILE_NAME = './Model/RF.bin'\n",
    "NN_SAVE_FILE_NAME = './Model/NN_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca5a3b0",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df646ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.34 s, sys: 563 ms, total: 3.9 s\n",
      "Wall time: 3.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "      <th>text_length</th>\n",
       "      <th>reduced_tags</th>\n",
       "      <th>text_token</th>\n",
       "      <th>text_token_stop</th>\n",
       "      <th>text_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 4 Purposes of Dreams</td>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "      <td>https://medium.com/science-for-real/the-4-purp...</td>\n",
       "      <td>['Eshan Samaranayake']</td>\n",
       "      <td>2020-12-21 16:05:19.524000+00:00</td>\n",
       "      <td>['Health', 'Neuroscience', 'Mental Health', 'P...</td>\n",
       "      <td>146</td>\n",
       "      <td>[Health, Mental Health, Psychology, Science]</td>\n",
       "      <td>[Passionate, about, the, synergy, between, sci...</td>\n",
       "      <td>[Passionate, synergy, science, technology, pro...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Surviving a Rod Through the Head</td>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "      <td>https://medium.com/live-your-life-on-purpose/s...</td>\n",
       "      <td>['Rishav Sinha']</td>\n",
       "      <td>2020-02-26 00:01:01.576000+00:00</td>\n",
       "      <td>['Brain', 'Health', 'Development', 'Psychology...</td>\n",
       "      <td>2326</td>\n",
       "      <td>[Health, Psychology, Science]</td>\n",
       "      <td>[You’ve, heard, of, him,, haven’t, you?, Phine...</td>\n",
       "      <td>[You’ve, heard, him,, haven’t, you?, Phineas, ...</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Quickly Build Trust, Tell Your Origin Story</td>\n",
       "      <td>Photo credit: Leo Leung People want to know wh...</td>\n",
       "      <td>https://medium.com/the-mission/want-trust-shar...</td>\n",
       "      <td>['Andy Raskin']</td>\n",
       "      <td>2016-07-06 19:45:00.648000+00:00</td>\n",
       "      <td>['Entrepreneurship', 'Personal Development', '...</td>\n",
       "      <td>4982</td>\n",
       "      <td>[Entrepreneurship, Personal Development, Start...</td>\n",
       "      <td>[Photo, credit:, Leo, Leung, People, want, to,...</td>\n",
       "      <td>[Photo, credit:, Leo, Leung, People, want, kno...</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facing Three Fundamental Coronavirus Fears</td>\n",
       "      <td>1. Since immunity to the novel coronavirus may...</td>\n",
       "      <td>https://medium.com/microbial-instincts/facing-...</td>\n",
       "      <td>['Bo Stapler']</td>\n",
       "      <td>2020-08-11 19:37:46.296000+00:00</td>\n",
       "      <td>['Health', 'Science', 'Wellness', 'Coronavirus...</td>\n",
       "      <td>3290</td>\n",
       "      <td>[Health, Science, Coronavirus, Covid 19]</td>\n",
       "      <td>[1., Since, immunity, to, the, novel, coronavi...</td>\n",
       "      <td>[1., Since, immunity, novel, coronavirus, may,...</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This 10-Minute Routine Will Increase Your Clar...</td>\n",
       "      <td>“Your subconscious mind works continuously, wh...</td>\n",
       "      <td>https://medium.com/the-mission/this-10-minute-...</td>\n",
       "      <td>['Benjamin Hardy']</td>\n",
       "      <td>2020-09-21 20:10:38.871000+00:00</td>\n",
       "      <td>['Productivity', 'Creativity', 'Motivation', '...</td>\n",
       "      <td>4446</td>\n",
       "      <td>[Productivity, Creativity, Motivation, Startup...</td>\n",
       "      <td>[“Your, subconscious, mind, works, continuousl...</td>\n",
       "      <td>[“Your, subconscious, mind, works, continuousl...</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                           The 4 Purposes of Dreams   \n",
       "1                   Surviving a Rod Through the Head   \n",
       "2     To Quickly Build Trust, Tell Your Origin Story   \n",
       "3         Facing Three Fundamental Coronavirus Fears   \n",
       "4  This 10-Minute Routine Will Increase Your Clar...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Passionate about the synergy between science a...   \n",
       "1  You’ve heard of him, haven’t you? Phineas Gage...   \n",
       "2  Photo credit: Leo Leung People want to know wh...   \n",
       "3  1. Since immunity to the novel coronavirus may...   \n",
       "4  “Your subconscious mind works continuously, wh...   \n",
       "\n",
       "                                                 url                 authors  \\\n",
       "0  https://medium.com/science-for-real/the-4-purp...  ['Eshan Samaranayake']   \n",
       "1  https://medium.com/live-your-life-on-purpose/s...        ['Rishav Sinha']   \n",
       "2  https://medium.com/the-mission/want-trust-shar...         ['Andy Raskin']   \n",
       "3  https://medium.com/microbial-instincts/facing-...          ['Bo Stapler']   \n",
       "4  https://medium.com/the-mission/this-10-minute-...      ['Benjamin Hardy']   \n",
       "\n",
       "                          timestamp  \\\n",
       "0  2020-12-21 16:05:19.524000+00:00   \n",
       "1  2020-02-26 00:01:01.576000+00:00   \n",
       "2  2016-07-06 19:45:00.648000+00:00   \n",
       "3  2020-08-11 19:37:46.296000+00:00   \n",
       "4  2020-09-21 20:10:38.871000+00:00   \n",
       "\n",
       "                                                tags  text_length  \\\n",
       "0  ['Health', 'Neuroscience', 'Mental Health', 'P...          146   \n",
       "1  ['Brain', 'Health', 'Development', 'Psychology...         2326   \n",
       "2  ['Entrepreneurship', 'Personal Development', '...         4982   \n",
       "3  ['Health', 'Science', 'Wellness', 'Coronavirus...         3290   \n",
       "4  ['Productivity', 'Creativity', 'Motivation', '...         4446   \n",
       "\n",
       "                                        reduced_tags  \\\n",
       "0       [Health, Mental Health, Psychology, Science]   \n",
       "1                      [Health, Psychology, Science]   \n",
       "2  [Entrepreneurship, Personal Development, Start...   \n",
       "3           [Health, Science, Coronavirus, Covid 19]   \n",
       "4  [Productivity, Creativity, Motivation, Startup...   \n",
       "\n",
       "                                          text_token  \\\n",
       "0  [Passionate, about, the, synergy, between, sci...   \n",
       "1  [You’ve, heard, of, him,, haven’t, you?, Phine...   \n",
       "2  [Photo, credit:, Leo, Leung, People, want, to,...   \n",
       "3  [1., Since, immunity, to, the, novel, coronavi...   \n",
       "4  [“Your, subconscious, mind, works, continuousl...   \n",
       "\n",
       "                                     text_token_stop  text_after  \n",
       "0  [Passionate, synergy, science, technology, pro...          12  \n",
       "1  [You’ve, heard, him,, haven’t, you?, Phineas, ...         234  \n",
       "2  [Photo, credit:, Leo, Leung, People, want, kno...         535  \n",
       "3  [1., Since, immunity, novel, coronavirus, may,...         297  \n",
       "4  [“Your, subconscious, mind, works, continuousl...         426  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# df = pd.read_csv('./data/medium_articles.csv')\n",
    "df = pd.read_pickle(DF_FILE_PATH)\n",
    "# print(final_df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "333b2027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78259, 11)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "# (192368, 6)\n",
    "# (119778, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b92426c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9c4e249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 24.1 ms, total: 1.89 s\n",
      "Wall time: 1.89 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       [Passionate, about, the, synergy, between, sci...\n",
       "1       [You’ve, heard, of, him,, haven’t, you?, Phine...\n",
       "2       [Photo, credit:, Leo, Leung, People, want, to,...\n",
       "3       [1., Since, immunity, to, the, novel, coronavi...\n",
       "4       [“Your, subconscious, mind, works, continuousl...\n",
       "                              ...                        \n",
       "9995    [This, article, is, based, on, Free, Code, Cam...\n",
       "9996    [Get, comfortable,, being, uncomfortable, What...\n",
       "9997    [Repeated, 51, percent, attacks, on, proof-of-...\n",
       "9998    [TSDN, Incentive, Program, In, order, to, boos...\n",
       "9999    [Easy, way, to, exchange, your, USD/EUR, to, X...\n",
       "Name: text_token, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('\\n\\n', ' '))\n",
    "df['text_token'] = df['text'].apply(lambda x: x.split(' '))\n",
    "df['text_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc807a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Passionate about the synergy between science a...\n",
       "1       You’ve heard of him, haven’t you? Phineas Gage...\n",
       "2       Photo credit: Leo Leung People want to know wh...\n",
       "3       1. Since immunity to the novel coronavirus may...\n",
       "4       “Your subconscious mind works continuously, wh...\n",
       "                              ...                        \n",
       "9995    This article is based on Free Code Camp Basic ...\n",
       "9996    Get comfortable, being uncomfortable What’s im...\n",
       "9997    Repeated 51 percent attacks on proof-of-work (...\n",
       "9998    TSDN Incentive Program In order to boost commu...\n",
       "9999    Easy way to exchange your USD/EUR to XRP with ...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ac81f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "tags = [tag for tag_list in df['reduced_tags'] for tag in tag_list ]\n",
    "print(len(list(set(tags))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69d54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef850830",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7261b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 有 selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eaade9d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # from models.decision_tree import TokenDecisionTreeModel\n",
    "\n",
    "# class TokenDecisionTreeModel_withSelection():\n",
    "#     def __init__(self, dt_args = dict(), tfidf_args = dict()):\n",
    "#         self.sklearn_model = tree.DecisionTreeClassifier(**dt_args)\n",
    "        \n",
    "#         if \"analyzer\" not in tfidf_args.keys():\n",
    "#             tfidf_args[\"analyzer\"] = lambda x : x\n",
    "#         if \"decode_error\" not in tfidf_args.keys():\n",
    "#             tfidf_args[\"decode_error\"] = \"ignore\"\n",
    "        \n",
    "#         self.enc = TfidfVectorizer(**tfidf_args)\n",
    "#         self.lenc = CountVectorizer(analyzer = lambda x : x, decode_error=\"ignore\")\n",
    "#         self.drop_keys = []\n",
    "        \n",
    "#     def list2onehot(self, l, mode=\"train\"):\n",
    "#         if mode==\"train\":\n",
    "#             self.enc.fit(l)\n",
    "#         return self.enc.transform(l).toarray()\n",
    "    \n",
    "#     def class2onehot(self, lc, mode=\"train\"):\n",
    "#         if mode==\"train\":\n",
    "#             self.lenc.fit(lc)\n",
    "#         return self.lenc.transform(lc).toarray()\n",
    "        \n",
    "#     def train(self, X, Y):\n",
    "#         ## X : list of tokens (n,t)\n",
    "#         ## Y : list of class (n,c)\n",
    "#         ## return token\n",
    "#         X_array = self.list2onehot(X)\n",
    "#         Y_array = self.class2onehot(Y)\n",
    "        \n",
    "#         ######## selection ####################\n",
    "#         threshold = 0.05\n",
    "#         X_array_df = pd.DataFrame(X_array)\n",
    "#         Y_array_df = pd.DataFrame(Y_array)\n",
    "        \n",
    "#         selected_features = [] \n",
    "#         for i in Y_array_df.keys():\n",
    "#             selector = SelectKBest(chi2, k='all')\n",
    "#             selector.fit(X_array, Y_array_df[i])\n",
    "#             selected_features.append(list(selector.scores_))\n",
    "    \n",
    "#         # MeanCS \n",
    "#         selected_features = np.mean(selected_features, axis=0) > threshold\n",
    "#         # MaxCS\n",
    "# #        selected_features = np.max(selected_features, axis=0) > threshold\n",
    "           \n",
    "#         for i in range(X_array_df.shape[1]):\n",
    "#             if selected_features[i] == False:\n",
    "#                 self.drop_keys.append(i)\n",
    "#         print(X_array_df.shape)\n",
    "#         X_array_df = X_array_df.drop(columns = self.drop_keys)\n",
    "#         print(X_array_df.shape) \n",
    "#         X_array = X_array_df.values.tolist()\n",
    "        \n",
    "#         ######## selection ###################\n",
    "        \n",
    "#         self.sklearn_model.fit(X_array, Y_array)\n",
    "#         feature_importance_list = list(zip([tup[0] for tup in sorted(list(self.enc.vocabulary_.items()), key=lambda x : x[1])], self.sklearn_model.feature_importances_.tolist()))\n",
    "#         feature_importance_list = sorted(feature_importance_list, key = lambda x : -x[1])\n",
    "#         return feature_importance_list\n",
    "    \n",
    "#     def predict(self, X, type=\"str\"):\n",
    "#         X_array = self.list2onehot(X, mode=\"test\")\n",
    "#         ##########  selection  ################\n",
    "#         X_array_df = pd.DataFrame(X_array)\n",
    "#         X_array_df = X_array_df.drop(columns = self.drop_keys)\n",
    "#         X_array = X_array_df.values.tolist()\n",
    "#         ###########   selection  #################\n",
    "        \n",
    "#         #Y_array = self.class2onehot(Y, mode=\"test\")\n",
    "#         pred = self.sklearn_model.predict(X_array)\n",
    "#         return self.lenc.inverse_transform(pred) if type==\"str\" else pred\n",
    "    \n",
    "#     def predict_proba(self, X):\n",
    "#         X_array = self.list2onehot(X, mode=\"test\")\n",
    "#         ##########   selection   ################\n",
    "#         X_array_df = pd.DataFrame(X_array)\n",
    "#         X_array_df = X_array_df.drop(columns = self.drop_keys)\n",
    "#         X_array = X_array_df.values.tolist()\n",
    "#         ###########   selection  #################\n",
    "#         #Y_array = self.class2onehot(Y, mode=\"test\")\n",
    "#         pred = self.sklearn_model.predict_proba(X_array)\n",
    "#         return np.array([r[:,1] for r in pred]).T\n",
    "    \n",
    "#     def score(self, X, Y, k=5):\n",
    "#         X_array = self.list2onehot(X, mode=\"test\")\n",
    "#         ##########  selection  ################\n",
    "#         X_array_df = pd.DataFrame(X_array)\n",
    "#         X_array_df = X_array_df.drop(columns = self.drop_keys)\n",
    "#         X_array = X_array_df.values.tolist()\n",
    "#         ###########  selection  #################\n",
    "        \n",
    "#         Y_array = self.class2onehot(Y, mode=\"test\")\n",
    "#         pred = self.predict(X, type=\"num\")\n",
    "        \n",
    "#         acc = self.sklearn_model.score(X_array, Y_array)\n",
    "#         precision = precision_score(Y_array, pred, average='micro')\n",
    "#         recall = recall_score(Y_array, pred, average='micro')\n",
    "#         pr_metrix = precision_recall_fscore_support(Y_array, pred, average=None)\n",
    "        \n",
    "#         pred_proba = self.predict_proba(X)\n",
    "#         topk_class = np.argsort(pred_proba, axis=1)[:,-k:]\n",
    "#         topk_pred = np.zeros_like(pred)\n",
    "        \n",
    "#         for i in range(pred.shape[0]):\n",
    "#             topk_pred[i, topk_class[i]] = 1\n",
    "        \n",
    "#         topk = recall_score(Y_array, topk_pred, average='micro')\n",
    "#         return acc, recall, precision, pr_metrix, topk\n",
    "#     def get_depth(self):\n",
    "#         return self.sklearn_model.get_depth()\n",
    "#     def get_leaves(self):\n",
    "#         return self.sklearn_model.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e845bb4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # from models.decision_tree import TokenDecisionTreeModel\n",
    "# #     model\n",
    "# #     self.enc = TfidfVectorizer\n",
    "# #     self.lenc = CountVectorizer\n",
    "\n",
    "# def score_tokenize_withSelection(df , token_col_name, class_col_name, print_mes = False, max_depth =None, min_samples_split = 2, test_size = 0.3, random_state = 42, threshold = 0.05):\n",
    "    \n",
    "#     train, test = train_test_split(df, test_size = test_size, random_state= random_state)\n",
    "    \n",
    "#     train_token_list = list(train[token_col_name])\n",
    "#     train_class_list = list(train[class_col_name])\n",
    "#     test_token_list = list(test[token_col_name])\n",
    "#     test_class_list = list(test[class_col_name])\n",
    "\n",
    "#     print(f'training dataset size: {len(train_token_list)}')\n",
    "#     print(f'testing dataset size: {len(test_token_list)}')\n",
    "#     model = TokenDecisionTreeModel_withSelection(dt_args={\"random_state\":42,\"max_depth\": max_depth, \"min_samples_split\":min_samples_split })\n",
    "    \n",
    "\n",
    "#     model.list2onehot(train_token_list)\n",
    "#     model.class2onehot(train_class_list)\n",
    "    \n",
    "#     print(f'x_columns_size: {len(model.enc.vocabulary_)}')\n",
    "#     print(f'y_columns_size: {len(model.lenc.vocabulary_)}')\n",
    "    \n",
    "#     # print(model.lenc.vocabulary_)\n",
    "    \n",
    "#     feature_importance_list = model.train(train_token_list, train_class_list)\n",
    "#     print(f'feature_importance_list: {len(feature_importance_list)}')\n",
    "#     print(f'feature_importance_list: {feature_importance_list[:30]}')\n",
    "    \n",
    "#     # predict\n",
    "#     predict_return_matrix_num = model.predict(test_token_list ,type='num')\n",
    "#     predict_return_matrix = model.predict(test_token_list)\n",
    "\n",
    "#     # score\n",
    "#     acc, recall, precision, pr_metrix, topk = model.score(test_token_list, test_class_list)\n",
    "#     score_df = pd.DataFrame([{'acc': acc, 'recall': recall, 'precision': precision, 'topk':topk}])\n",
    "\n",
    "#     # 印出一些東東\n",
    "#     if print_mes:   \n",
    "#         acutual_average = model.class2onehot(df[class_col_name]).sum(axis=1).mean()\n",
    "#         acutual_distribution = model.class2onehot(df[class_col_name]).sum(axis=0)\n",
    "#         print(f'原始資料： {class_col_name} 數目：{acutual_average}')\n",
    "#         print(f'原始資料：每個 {class_col_name} 被使用之數目：{acutual_distribution}')\n",
    "#         print(f'實際{class_col_name}數 {len(acutual_distribution)}')\n",
    "        \n",
    "#         predict_average = predict_return_matrix_num.sum(axis=1).mean()\n",
    "#         predict_distribution = predict_return_matrix_num.sum(axis=0)\n",
    "#         print(f'預測中：平均 {class_col_name} 數目：{predict_average}')\n",
    "#         print(f'預測中：每個 {class_col_name} 被預測之數目：{predict_distribution}')\n",
    "#         print(f'預測{class_col_name}數 {len(predict_distribution)}')\n",
    "        \n",
    "#         print(f'{class_col_name} model 深度: {model.get_depth()}, 葉子數：{model.get_leaves()}')\n",
    "        \n",
    "#         error_cnt = 0\n",
    "#         for i in range(len(test_class_list)):\n",
    "#             if len(test_class_list[i]) == len(predict_return_matrix[i]):\n",
    "#                 if test_class_list[i].sort() == predict_return_matrix[i].sort():\n",
    "#                     continue\n",
    "#             error_cnt+=1\n",
    "#             print(f'predict: {predict_return_matrix[i]} , true: {test_class_list[i]}')\n",
    "#         print(error_cnt)\n",
    "    \n",
    "#     return model, score_df.T, pr_metrix, predict_return_matrix, predict_return_matrix_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1bcd19",
   "metadata": {},
   "source": [
    "## 沒有 selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd3c0c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.decision_tree import TokenDecisionTreeModel\n",
    "class TokenDecisionTreeModel():\n",
    "    def __init__(self, dt_args = dict(), tfidf_args = dict()):\n",
    "        self.sklearn_model = tree.DecisionTreeClassifier(**dt_args)\n",
    "        if \"analyzer\" not in tfidf_args.keys():\n",
    "            tfidf_args[\"analyzer\"] = lambda x : x\n",
    "        if \"decode_error\" not in tfidf_args.keys():\n",
    "            tfidf_args[\"decode_error\"] = \"ignore\"\n",
    "        self.enc = TfidfVectorizer(**tfidf_args)\n",
    "        self.lenc = CountVectorizer(analyzer = lambda x : x, decode_error=\"ignore\")\n",
    "        \n",
    "    def list2onehot(self, l, mode=\"train\"):\n",
    "        if mode==\"train\":\n",
    "            self.enc.fit(l)\n",
    "        return self.enc.transform(l).toarray()\n",
    "    \n",
    "    def class2onehot(self, lc, mode=\"train\"):\n",
    "        if mode==\"train\":\n",
    "            self.lenc.fit(lc)\n",
    "        return self.lenc.transform(lc).toarray()\n",
    "        \n",
    "    def train(self, X, Y):\n",
    "        ## X : list of tokens (n,t)\n",
    "        ## Y : list of class (n,c)\n",
    "        ## return token\n",
    "        X_array = self.list2onehot(X)\n",
    "        print('finish list2onehot')\n",
    "        Y_array = self.class2onehot(Y)\n",
    "        print('finish class2onehot')\n",
    "        self.sklearn_model.fit(X_array, Y_array)\n",
    "        print('finish train')\n",
    "        feature_importance_list = list(zip([tup[0] for tup in sorted(list(self.enc.vocabulary_.items()), key=lambda x : x[1])], self.sklearn_model.feature_importances_.tolist()))\n",
    "        feature_importance_list = sorted(feature_importance_list, key = lambda x : -x[1])\n",
    "\n",
    "        return feature_importance_list\n",
    "    \n",
    "    def predict(self, X, type=\"str\"):\n",
    "        X_array = self.list2onehot(X, mode=\"test\")\n",
    "        #Y_array = self.class2onehot(Y, mode=\"test\")\n",
    "        pred = self.sklearn_model.predict(X_array)\n",
    "        return self.lenc.inverse_transform(pred) if type==\"str\" else pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_array = self.list2onehot(X, mode=\"test\")\n",
    "        #Y_array = self.class2onehot(Y, mode=\"test\")\n",
    "        pred = self.sklearn_model.predict_proba(X_array)\n",
    "        return np.array([r[:,1] for r in pred]).T\n",
    "    \n",
    "    def score(self, X, Y, k=5):\n",
    "        X_array = self.list2onehot(X, mode=\"test\")\n",
    "        Y_array = self.class2onehot(Y, mode=\"test\")\n",
    "        pred = self.predict(X, type=\"num\")\n",
    "        \n",
    "        acc = self.sklearn_model.score(X_array, Y_array)\n",
    "        precision = precision_score(Y_array, pred, average='micro')\n",
    "        recall = recall_score(Y_array, pred, average='micro')\n",
    "        pr_metrix = precision_recall_fscore_support(Y_array, pred, average=None)\n",
    "        \n",
    "        pred_proba = self.predict_proba(X)\n",
    "        topk_class = np.argsort(pred_proba, axis=1)[:,-k:]\n",
    "        topk_pred = np.zeros_like(pred)\n",
    "        for i in range(pred.shape[0]):\n",
    "            topk_pred[i, topk_class[i]] = 1\n",
    "        \n",
    "        topk = recall_score(Y_array, topk_pred, average='micro')\n",
    "        return acc, recall, precision, pr_metrix, topk\n",
    "    def get_depth(self):\n",
    "        return self.sklearn_model.get_depth()\n",
    "    def get_leaves(self):\n",
    "        return self.sklearn_model.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "530d9af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.decision_tree import TokenDecisionTreeModel\n",
    "#     model\n",
    "#     self.enc = TfidfVectorizer\n",
    "#     self.lenc = CountVectorizer\n",
    "\n",
    "def score_tokenize(df, token_col_name, class_col_name, print_mes = False, max_depth =None, min_samples_split = 2, test_size = 0.3, random_state = 42, threshold = 0.05):\n",
    "    \n",
    "    train, test = train_test_split(df, test_size = test_size, random_state= random_state)\n",
    "    \n",
    "    train_token_list = list(train[token_col_name])\n",
    "    train_class_list = list(train[class_col_name])\n",
    "    test_token_list = list(test[token_col_name])\n",
    "    test_class_list = list(test[class_col_name])\n",
    "    \n",
    "    \n",
    "    print(f'training dataset size: {len(train_token_list)}')\n",
    "    print(f'testing dataset size: {len(test_token_list)}')\n",
    "    model = TokenDecisionTreeModel(dt_args={\"random_state\":42,\"max_depth\": max_depth, \"min_samples_split\":min_samples_split })\n",
    "    model.list2onehot(train_token_list)\n",
    "    model.class2onehot(train_class_list)\n",
    "    \n",
    "    print(f'x_columns_size: {len(model.enc.vocabulary_)}')\n",
    "    print(f'y_columns_size: {len(model.lenc.vocabulary_)}')\n",
    "    \n",
    "    # print(model.lenc.vocabulary_)\n",
    "    \n",
    "    feature_importance_list = model.train(train_token_list, train_class_list)\n",
    "    print(f'feature_importance_list: {len(feature_importance_list)}')\n",
    "    print(f'feature_importance_list: {feature_importance_list[:30]}')\n",
    "    \n",
    "    # predict\n",
    "    predict_return_matrix_num = model.predict(test_token_list ,type='num')\n",
    "    predict_return_matrix = model.predict(test_token_list)\n",
    "\n",
    "    # score\n",
    "    acc, recall, precision, pr_metrix, topk = model.score(test_token_list, test_class_list)\n",
    "    score_df = pd.DataFrame([{'acc': acc, 'recall': recall, 'precision': precision, 'topk':topk}])\n",
    "    score_df.index = [class_col_name+'one_DT']\n",
    "\n",
    "    ## 計算空的\n",
    "    test_class_list_class2onehot = model.class2onehot(test_class_list)\n",
    "    print(f'test_class_list_class2onehot {len(test_class_list_class2onehot)}')\n",
    "    empty = 0\n",
    "    for i in test_class_list_class2onehot:\n",
    "        if sum(i) == 0:\n",
    "            empty+=1\n",
    "    print(f'empty {empty}')\n",
    "    \n",
    "    # 印出一些東東\n",
    "    if print_mes:   \n",
    "        acutual_average = model.class2onehot(df[class_col_name]).sum(axis=1).mean()\n",
    "        acutual_distribution = model.class2onehot(df[class_col_name]).sum(axis=0)\n",
    "        print(f'原始資料： {class_col_name} 數目：{acutual_average}')\n",
    "        print(f'原始資料：每個 {class_col_name} 被使用之數目：{acutual_distribution}')\n",
    "        print(f'實際{class_col_name}數 {len(acutual_distribution)}')\n",
    "        \n",
    "        predict_average = predict_return_matrix_num.sum(axis=1).mean()\n",
    "        predict_distribution = predict_return_matrix_num.sum(axis=0)\n",
    "        print(f'預測中：平均 {class_col_name} 數目：{predict_average}')\n",
    "        print(f'預測中：每個 {class_col_name} 被預測之數目：{predict_distribution}')\n",
    "        print(f'預測{class_col_name}數 {len(predict_distribution)}')\n",
    "        \n",
    "        print(f'{class_col_name} model 深度: {model.get_depth()}, 葉子數：{model.get_leaves()}')\n",
    "        \n",
    "        error_cnt = 0\n",
    "        for i in range(len(test_class_list)):\n",
    "            if len(test_class_list[i]) == len(predict_return_matrix[i]):\n",
    "                if test_class_list[i].sort() == predict_return_matrix[i].sort():\n",
    "                    continue\n",
    "            error_cnt+=1\n",
    "            print(f'predict: {predict_return_matrix[i]} , true: {test_class_list[i]}')\n",
    "        print(error_cnt)\n",
    "    \n",
    "    return model, score_df.T, pr_metrix, predict_return_matrix, predict_return_matrix_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3451d83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size: 800\n",
      "testing dataset size: 200\n",
      "x_columns_size: 41699\n",
      "y_columns_size: 92\n",
      "finish list2onehot\n",
      "finish class2onehot\n",
      "finish train\n",
      "feature_importance_list: 41699\n",
      "feature_importance_list: [('writing', 0.04834943319448813), ('AI', 0.01901943981600437), ('business', 0.015917222590759086), ('data', 0.013944278151188799), ('of', 0.010362192151440961), ('code', 0.009682142850322697), ('what', 0.0090327236430647), ('this', 0.008748204746430224), ('don’t', 0.0083280284477433), ('design', 0.008218636289750102), ('write', 0.008049799677316142), ('content', 0.007720222527187995), ('Covid-19', 0.0077140929706380566), ('The', 0.0074387483012067195), ('I', 0.0073175281347155905), ('during', 0.007141138823036827), ('in', 0.006850143387877952), ('mental', 0.006834345332927698), ('market', 0.006821444855491488), ('model', 0.006796717127275867), ('writer', 0.006770472870692443), ('for', 0.006698533031414467), ('and', 0.006550392053003021), ('COVID-19', 0.006450920757200824), ('eat', 0.00629190848496982), ('is', 0.006266854282469228), ('do', 0.0061431409345117805), ('the', 0.005994951131266982), ('code.', 0.005868873133974519), ('would', 0.0057875652629685225)]\n",
      "test_class_list_class2onehot 200\n",
      "empty 0\n",
      "原始資料： reduced_tags 數目：3.434\n",
      "原始資料：每個 reduced_tags 被使用之數目：[ 76  18  17 112   2   5  44  90   5   2   2  25  10  41  30 254   1  23\n",
      "   5  82  31  17  59  11  22 102  26   3   1  10   4   1  13   4 149   8\n",
      "  13  15  31   1  28   1  10   5  51  52  18   3  75  95  94   7  15  30\n",
      "   1  35   5   5   3  13  12   2  25   2   7 154 104  94  76   1  22   8\n",
      "   2  92  53  63  25  10  37  33  57  61   7   1 109  26  12  74   1   1\n",
      "  14   6   1  29 258  44]\n",
      "實際reduced_tags數 96\n",
      "預測中：平均 reduced_tags 數目：3.295\n",
      "預測中：每個 reduced_tags 被預測之數目：[14  4  2 20  0  0 11 11  2  0  2  5  1  6 11 53  0  5  0 20  5  2  7  2\n",
      "  1 26  6  0  2  0  4  1 31  0  5  3  5 11  1  2  2  9 10  2  1 13 15 24\n",
      "  1  2  1  0  6  2  1  0  2  1  0  9  0  5 25 30 12 12  9  4  0 21  5 10\n",
      "  3  1  4  5  4  4  0  0 17  5  4 12  0  4  0  1  1  8 54 17]\n",
      "預測reduced_tags數 92\n",
      "reduced_tags model 深度: 65, 葉子數：731\n",
      "predict: ['Books' 'Humor' 'Music' 'Social Media'] , true: ['Life Lessons', 'Inspiration', 'Personal Development', 'Creativity', 'Productivity']\n",
      "predict: ['Creativity' 'Writing' 'Writing Tips'] , true: ['Marketing', 'Creativity', 'Writing Tips', 'Inspiration', 'Writing']\n",
      "predict: ['Business' 'Entrepreneurship' 'Startup'] , true: ['Marketing']\n",
      "predict: ['Health' 'Mental Health' 'Relationships'] , true: ['Business', 'Startup', 'Leadership', 'Life Lessons', 'Entrepreneurship']\n",
      "predict: ['Creativity' 'Writing' 'Writing Tips'] , true: ['Self', 'Music', 'Mental Health', 'Pandemic']\n",
      "predict: ['Business' 'Creativity' 'Marketing' 'Writing'] , true: ['Software Engineering', 'Software Development']\n",
      "predict: ['JavaScript' 'Programming' 'React' 'Software Engineering'] , true: ['Inspiration', 'Music']\n",
      "predict: ['AI' 'Artificial Intelligence' 'Data Science' 'Deep Learning'\n",
      " 'Machine Learning'] , true: ['Machine Learning', 'Artificial Intelligence', 'Data Science', 'Deep Learning']\n",
      "predict: ['Entrepreneurship' 'Food' 'Innovation' 'Marketing'] , true: ['Productivity', 'Creativity']\n",
      "predict: ['Life' 'Life Lessons' 'Mental Health' 'Productivity' 'Psychology'] , true: ['Writing', 'Writing Tips', 'Creativity']\n",
      "predict: ['Life' 'Life Lessons' 'Storytelling'] , true: ['Society', 'Coronavirus']\n",
      "predict: ['Storytelling'] , true: ['Music', 'Science']\n",
      "predict: ['Books' 'Humor' 'Music' 'Social Media'] , true: ['Health', 'Environment']\n",
      "predict: ['Climate Change' 'Environment'] , true: ['Life Lessons', 'Creativity', 'Psychology', 'Science']\n",
      "predict: ['Data Visualization'] , true: ['Machine Learning', 'Deep Learning', 'Artificial Intelligence', 'Data Science']\n",
      "predict: ['Entrepreneurship' 'Health' 'Productivity' 'Science' 'Self Improvement'] , true: ['Python', 'Marketing', 'Programming', 'Writing']\n",
      "predict: ['Creativity' 'Writing' 'Writing Tips'] , true: ['Creativity', 'Writing Tips', 'Productivity', 'Writing']\n",
      "predict: ['Marketing' 'Productivity' 'Psychology' 'Writing'] , true: ['Social Media', 'Technology', 'Writing', 'Business', 'Entrepreneurship']\n",
      "predict: ['Creativity' 'Fiction' 'Productivity' 'Writing' 'Writing Tips'] , true: ['Writing', 'Marketing', 'Entrepreneurship']\n",
      "predict: ['Data Science' 'Programming'] , true: ['Python', 'Data Visualization', 'Science']\n",
      "predict: ['Coronavirus' 'Poetry'] , true: ['Poetry', 'Writing', 'Creativity', 'Life']\n",
      "predict: ['Life' 'Life Lessons' 'Mental Health' 'Productivity' 'Self-awareness'] , true: ['Self-awareness', 'Self', 'Mental Health', 'Productivity']\n",
      "predict: ['Creativity' 'Writing' 'Writing Tips'] , true: ['Motivation', 'Writing', 'Life', 'Writing Tips', 'Creativity']\n",
      "predict: ['Creativity' 'Productivity' 'Writing' 'Writing Tips'] , true: ['Health', 'Lifestyle']\n",
      "predict: ['JavaScript' 'Programming' 'React'] , true: ['Tech']\n",
      "predict: ['Covid 19' 'Entrepreneurship'] , true: ['Productivity', 'Creativity', 'Writing', 'Psychology']\n",
      "predict: ['Creativity' 'Culture' 'Leadership' 'Mental Health' 'Mindfulness'] , true: ['Self Improvement', 'Writing', 'Psychology', 'Creativity']\n",
      "predict: ['JavaScript' 'Programming' 'React'] , true: ['Politics', 'Health', 'Coronavirus', 'Science']\n",
      "predict: ['Machine Learning' 'Programming' 'Python'] , true: ['Machine Learning', 'Python']\n",
      "predict: ['Art' 'Machine Learning' 'Python'] , true: ['Self', 'Mental Health', 'Relationships', 'Psychology', 'Society']\n",
      "predict: ['Climate Change' 'Environment'] , true: ['Storytelling', 'Climate Change', 'Psychology', 'Film', 'Environment']\n",
      "predict: ['Creativity' 'Personal Development' 'Productivity' 'Self' 'Work'] , true: ['Life', 'Lifestyle', 'Health', 'Motivation']\n",
      "predict: ['Entrepreneurship' 'Innovation' 'Startup' 'Technology'] , true: ['Technology', 'Artificial Intelligence', 'AI']\n",
      "predict: ['Creativity' 'Humor' 'Writing'] , true: ['Entrepreneurship', 'Business', 'Startup', 'Productivity']\n",
      "predict: ['Creativity' 'Life' 'Music' 'Pandemic' 'Writing'] , true: ['Creativity', 'Advice', 'Inspiration', 'Writing']\n",
      "predict: ['Books' 'Careers' 'Coding' 'Programming'] , true: ['Writing', 'Creativity', 'Motivation', 'Self-awareness', 'Art']\n",
      "predict: ['Design' 'Marketing' 'Startup' 'Web Development'] , true: ['Motivation', 'Inspiration', 'Productivity', 'Writing', 'Creativity']\n",
      "predict: ['Creativity' 'Self' 'Social Media' 'Writing'] , true: ['Life', 'Writing', 'Life Lessons', 'Inspiration', 'Creativity']\n",
      "predict: ['Creativity' 'Humor' 'Writing'] , true: ['Psychology', 'Artificial Intelligence', 'Machine Learning', 'Science']\n",
      "predict: ['Storytelling'] , true: ['Art', 'Life Lessons', 'Mental Health', 'Storytelling', 'Creativity']\n",
      "predict: ['Creativity' 'Writing'] , true: ['Programming', 'AI', 'Python', 'Machine Learning']\n",
      "predict: ['Philosophy' 'Science' 'Technology'] , true: ['Business']\n",
      "predict: ['Data Visualization' 'Design' 'Health' 'Storytelling'] , true: ['Entrepreneurship', 'Technology', 'Social Media', 'Productivity', 'Writing']\n",
      "predict: ['Food' 'Health' 'Lifestyle' 'Science'] , true: ['Climate Change']\n",
      "predict: ['Artificial Intelligence' 'Data Science' 'Machine Learning' 'Science'] , true: ['Data Science', 'Machine Learning', 'Artificial Intelligence', 'AI', 'Careers']\n",
      "predict: ['Politics' 'Science' 'Tech' 'Technology'] , true: ['Philosophy', 'Climate Change', 'Science']\n",
      "predict: ['Python'] , true: ['Data Science', 'Python', 'Software Development', 'Programming', 'Artificial Intelligence']\n",
      "predict: ['Productivity' 'Programming' 'Software Development' 'Startup'] , true: ['Software Development', 'Productivity']\n",
      "predict: ['Health' 'Science'] , true: ['Artificial Intelligence', 'Education', 'Machine Learning']\n",
      "predict: ['Data Science' 'Programming'] , true: ['Covid 19', 'Health', 'Science', 'Coronavirus']\n",
      "predict: ['Food' 'Health' 'Mental Health' 'Psychology' 'Self'] , true: ['Psychology', 'Climate Change', 'Environment']\n",
      "predict: ['Books' 'Creativity' 'Writing' 'Writing Tips'] , true: ['Entrepreneurship', 'Self-awareness', 'Productivity', 'Money', 'Writing']\n",
      "predict: ['AI' 'Artificial Intelligence' 'Data Science' 'Programming' 'Python'] , true: ['Python', 'Software Engineering']\n",
      "predict: ['Creativity' 'Short Story' 'Writing'] , true: ['Software Engineering', 'Software Development', 'Mental Health', 'Productivity', 'Programming']\n",
      "predict: ['AI' 'Artificial Intelligence' 'Technology'] , true: ['Self', 'Health', 'Science', 'Psychology']\n",
      "predict: ['Creativity' 'Entrepreneurship' 'Productivity' 'Work' 'Writing'] , true: ['Poetry', 'Health', 'Mental Health', 'Creativity']\n",
      "predict: ['AI' 'Artificial Intelligence' 'Machine Learning' 'Programming'] , true: ['Machine Learning', 'Technology', 'Artificial Intelligence', 'Python', 'AI']\n",
      "predict: ['Creativity' 'Writing' 'Writing Tips'] , true: ['Writing', 'Creativity']\n",
      "predict: ['Data Science' 'Programming'] , true: ['Marketing', 'Entrepreneurship', 'Startup', 'Tech']\n",
      "predict: ['Covid 19' 'Health' 'Mental Health' 'Trump'] , true: ['Machine Learning', 'Python', 'Artificial Intelligence', 'Software Development', 'Programming']\n",
      "predict: ['Creativity' 'Writing'] , true: ['Creativity', 'Self', 'Psychology', 'Inspiration', 'Writing']\n",
      "predict: ['JavaScript' 'Programming' 'Python' 'Startup'] , true: ['Mental Health', 'Christianity', 'Health']\n",
      "predict: ['Creativity' 'Fiction' 'Productivity' 'Writing' 'Writing Tips'] , true: ['Business', 'Writing', 'Productivity', 'Startup']\n",
      "predict: ['Business' 'Entrepreneurship' 'Startup'] , true: ['Design', 'Entrepreneurship', 'Social Media', 'Productivity', 'Creativity']\n",
      "predict: ['Science'] , true: ['Science', 'Covid 19', 'Health', 'Coronavirus']\n",
      "predict: ['Data Science' 'Programming'] , true: ['Python', 'Data Science', 'Programming', 'Data Visualization']\n",
      "predict: ['Science' 'Technology'] , true: ['Environment', 'Design', 'Data Visualization']\n",
      "predict: ['Christmas' 'Health' 'Mental Health'] , true: ['Humor', 'Health', 'Covid 19', 'Coronavirus']\n",
      "predict: ['JavaScript' 'Programming'] , true: ['UX', 'Creativity', 'Design', 'Technology']\n",
      "predict: ['Covid 19' 'Entrepreneurship'] , true: ['Machine Learning', 'Artificial Intelligence', 'AI', 'Data Science']\n",
      "predict: ['Data Visualization' 'Python'] , true: ['Python', 'Data', 'Data Science', 'Machine Learning']\n",
      "predict: ['Programming' 'Software Development' 'Software Engineering'] , true: ['Software Engineering', 'Programming', 'Software Development', 'Startup']\n",
      "predict: ['Politics' 'Science' 'Tech' 'Technology'] , true: ['Climate Change']\n",
      "predict: ['Health' 'Life' 'Mental Health'] , true: ['Life Lessons', 'Productivity', 'Creativity', 'Mindfulness']\n",
      "predict: ['Health' 'Life' 'Life Lessons' 'Mental Health' 'Self'] , true: ['Society', 'Life Lessons', 'Mental Health', 'Self']\n",
      "predict: ['Books' 'Inspiration' 'Music' 'Writing'] , true: ['Poetry', 'Books', 'Writing', 'Love', 'Creativity']\n",
      "predict: ['Creativity' 'Poetry' 'Writing'] , true: ['Writing Tips', 'Self-awareness', 'Books', 'Self Improvement', 'Writing']\n",
      "predict: ['AI' 'Artificial Intelligence' 'Data Science' 'Deep Learning'] , true: ['Technology', 'Artificial Intelligence', 'Society']\n",
      "predict: ['Coronavirus' 'Covid 19' 'Health'] , true: ['Machine Learning', 'Data Science']\n",
      "predict: ['Climate Change' 'Environment'] , true: ['Storytelling', 'Writing', 'Creativity']\n",
      "predict: ['Books' 'Entrepreneurship' 'Innovation' 'Motivation' 'Work'] , true: ['Social Media', 'Marketing', 'Society']\n",
      "predict: ['Entrepreneurship' 'Health' 'Leadership' 'Startup' 'Writing'] , true: ['Storytelling', 'Writing', 'Short Story', 'Psychology']\n",
      "predict: ['Advice' 'Creativity' 'Writing'] , true: ['Creativity', 'Life', 'Film', 'Writing']\n",
      "predict: ['Poetry' 'Writing'] , true: ['Music']\n",
      "predict: ['AI' 'Artificial Intelligence' 'Data Science' 'Machine Learning'\n",
      " 'Technology'] , true: ['Machine Learning', 'Technology', 'Artificial Intelligence', 'AI']\n",
      "predict: ['Creativity' 'Writing'] , true: ['Life Lessons', 'Entrepreneurship', 'Business', 'Startup']\n",
      "predict: ['Health' 'Mental Health'] , true: ['Mental Health', 'Health', 'Self Improvement', 'Education']\n",
      "predict: ['JavaScript' 'Programming' 'React'] , true: ['Startup']\n",
      "predict: ['Mental Health' 'Psychology' 'Society' 'Work'] , true: ['Entrepreneurship', 'Business', 'Startup']\n",
      "predict: ['Food' 'Health' 'Science'] , true: ['Spirituality', 'Writing', 'Religion', 'Health']\n",
      "predict: ['Creativity' 'Life Lessons' 'Writing' 'Writing Tips'] , true: ['Self Improvement', 'Writing', 'Creativity']\n",
      "predict: ['Creativity' 'Life Lessons' 'Self Improvement' 'Writing'] , true: ['Creativity', 'Relationships', 'Self Improvement', 'Productivity', 'Inspiration']\n",
      "predict: ['Design' 'Programming' 'Software Development' 'Technology'] , true: ['Python', 'Coding', 'Startup', 'Software Engineering', 'Programming']\n",
      "predict: ['Covid 19' 'Health' 'Mental Health' 'Trump'] , true: ['JavaScript', 'Python']\n",
      "predict: ['Mental Health' 'Psychology' 'Self-awareness'] , true: ['Marketing', 'Startup', 'Business', 'Entrepreneurship']\n",
      "predict: ['Python'] , true: ['Environment', 'Climate Change']\n",
      "predict: ['Entrepreneurship' 'Marketing' 'Startup'] , true: ['Software Engineering', 'Software Development', 'Python', 'Data Science', 'Programming']\n",
      "predict: ['AI' 'Artificial Intelligence' 'Digital Marketing'] , true: ['Artificial Intelligence', 'AI']\n",
      "predict: ['Covid 19' 'Health' 'Mental Health' 'Trump'] , true: ['Fiction', 'Writing', 'Creativity']\n",
      "predict: ['Entrepreneurship' 'Learning' 'Programming' 'Startup'] , true: ['Investing', 'Entrepreneurship', 'Business', 'Technology', 'Productivity']\n",
      "predict: ['Creativity' 'Poetry' 'Writing'] , true: ['Startup', 'Writing']\n",
      "predict: ['JavaScript' 'Programming' 'React'] , true: ['Psychology', 'Business', 'Money', 'Marketing']\n",
      "predict: ['Creativity' 'Writing' 'Writing Tips'] , true: ['Inspiration', 'Creativity', 'Productivity', 'Writing']\n",
      "predict: ['Books' 'Culture' 'News' 'Politics' 'Society'] , true: ['Climate Change', 'Technology', 'Science']\n",
      "predict: ['Creativity' 'Poetry' 'Writing'] , true: ['Writing', 'Social Media', 'Marketing', 'Productivity', 'Money']\n",
      "predict: ['Python'] , true: ['Business', 'Technology', 'Self Improvement', 'Science', 'Writing']\n",
      "predict: ['Science' 'Technology'] , true: ['Writing', 'Social Media', 'Marketing', 'Productivity', 'Creativity']\n",
      "predict: ['Data Science'] , true: ['Technology', 'Data Science']\n",
      "predict: ['Life' 'Life Lessons' 'Mental Health' 'Productivity' 'Self-awareness'] , true: ['Business', 'Entrepreneurship', 'Productivity']\n",
      "predict: ['Advice' 'Creativity' 'Writing' 'Writing Tips'] , true: ['Society', 'Writing', 'Racism']\n",
      "predict: ['Artificial Intelligence' 'Data Science' 'Machine Learning' 'Science'] , true: ['Machine Learning', 'Artificial Intelligence', 'Data Science']\n",
      "predict: ['Business' 'Entrepreneurship' 'Work' 'Writing'] , true: ['Writing', 'Creativity']\n",
      "predict: ['Creativity' 'Life' 'Mental Health' 'Poetry' 'Relationships'] , true: ['Design', 'Startup']\n",
      "predict: ['Artificial Intelligence' 'Data Science' 'Productivity' 'Programming'\n",
      " 'Python'] , true: ['Python', 'Finance']\n",
      "predict: ['Creativity' 'Writing' 'Writing Tips'] , true: ['Startup', 'Life Lessons', 'Self', 'Productivity', 'Creativity']\n",
      "predict: ['Entrepreneurship' 'Health' 'Productivity' 'Science' 'Self Improvement'] , true: ['Mental Health', 'Health', 'Psychology']\n",
      "predict: ['Artificial Intelligence' 'Culture'] , true: ['Self Improvement', 'Psychology', 'Self', 'Motivation', 'Health']\n",
      "predict: ['Money' 'Science' 'Society'] , true: ['Music', 'Design']\n",
      "predict: ['Artificial Intelligence' 'Machine Learning' 'Psychology' 'Science'] , true: ['Health', 'Science']\n",
      "predict: ['Careers' 'Creativity' 'Entrepreneurship' 'Marketing'] , true: ['Marketing', 'Digital Marketing', 'Social Media']\n",
      "predict: ['Entrepreneurship' 'Mental Health' 'Psychology' 'Startup'] , true: ['Productivity', 'Startup', 'Entrepreneurship']\n",
      "predict: ['Storytelling'] , true: ['Software Development', 'Artificial Intelligence', 'Software Engineering', 'AI']\n",
      "predict: ['Health'] , true: ['Health', 'Self Improvement', 'Psychology', 'Life Lessons', 'Writing']\n",
      "predict: ['Creativity' 'Personal Development' 'Productivity' 'Self Improvement'\n",
      " 'Work'] , true: ['Work', 'Personal Development', 'Creativity', 'Productivity']\n",
      "predict: ['Programming'] , true: ['Productivity', 'Business', 'Writing', 'Short Story', 'Technology']\n",
      "predict: ['Entrepreneurship' 'Health' 'Productivity' 'Science' 'Self Improvement'] , true: ['Marketing', 'Leadership', 'Business', 'Productivity']\n",
      "predict: ['Business' 'Marketing' 'Startup'] , true: ['Health', 'Design']\n",
      "predict: ['Tech'] , true: ['Artificial Intelligence', 'Data Science', 'Technology']\n",
      "predict: ['Climate Change' 'Design' 'Environment'] , true: ['Climate Change', 'Environment']\n",
      "predict: ['Artificial Intelligence' 'Machine Learning'] , true: ['Technology']\n",
      "predict: ['Books' 'Humor' 'Music'] , true: ['Feminism', 'Social Media', 'Psychology', 'Writing', 'Mental Health']\n",
      "predict: ['Covid 19' 'Health' 'Mental Health' 'Trump'] , true: ['Books', 'Creativity', 'Self Improvement']\n",
      "predict: ['Creativity' 'Writing'] , true: ['Software Engineering', 'JavaScript', 'React', 'Programming', 'Web Development']\n",
      "predict: ['Python'] , true: ['Machine Learning', 'Artificial Intelligence', 'Education', 'Data Science']\n",
      "predict: ['Coronavirus' 'Covid 19' 'Health'] , true: ['Covid 19', 'Health', 'Science', 'Coronavirus']\n",
      "predict: ['Coronavirus' 'Covid 19' 'Health'] , true: ['Machine Learning', 'Python', 'Artificial Intelligence', 'AI', 'Programming']\n",
      "predict: ['Books' 'Data Science' 'Startup'] , true: ['Health', 'Design']\n",
      "predict: ['Creativity' 'Education' 'Learning' 'Productivity' 'Self Improvement'] , true: ['Health', 'Science']\n",
      "138\n",
      "CPU times: user 5.66 s, sys: 158 ms, total: 5.82 s\n",
      "Wall time: 5.81 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/lalami/lalami_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/adam/lalami/lalami_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reduced_tagsone_DT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>acc</th>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.239538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.251897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topk</th>\n",
       "      <td>0.259740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           reduced_tagsone_DT\n",
       "acc                  0.020000\n",
       "recall               0.239538\n",
       "precision            0.251897\n",
       "topk                 0.259740"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model, score_df, pr_metrix, predict_return_matrix, predict_return_matrix_num \\\n",
    "            = score_tokenize(df, TEXT_TOKEN_COL_NAME, CLASS_COL_NAME,\\\n",
    "                             test_size = TEST_SIZE, random_state = RANDOM_STATE, \\\n",
    "                             print_mes= True) \n",
    "with open(DT_SAVE_FILE_NAME, 'wb') as files:\n",
    "    pickle.dump(model.sklearn_model, files)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17dd30",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256f67c",
   "metadata": {},
   "source": [
    "##  沒 selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c48978d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class RandomForestModel():\n",
    "    def __init__(self, dt_args = dict(), tfidf_args = dict()):\n",
    "        self.sklearn_model = RandomForestClassifier(**dt_args)\n",
    "        if \"analyzer\" not in tfidf_args.keys():\n",
    "            tfidf_args[\"analyzer\"] = lambda x : x\n",
    "        if \"decode_error\" not in tfidf_args.keys():\n",
    "            tfidf_args[\"decode_error\"] = \"ignore\"\n",
    "        self.enc = TfidfVectorizer(**tfidf_args)\n",
    "        self.lenc = CountVectorizer(analyzer = lambda x : x, decode_error=\"ignore\")\n",
    "        \n",
    "    def list2onehot(self, l, mode=\"train\"):\n",
    "        if mode==\"train\":\n",
    "            self.enc.fit(l)\n",
    "        return self.enc.transform(l).toarray()\n",
    "    \n",
    "    def class2onehot(self, lc, mode=\"train\"):\n",
    "        if mode==\"train\":\n",
    "            self.lenc.fit(lc)\n",
    "        return self.lenc.transform(lc).toarray()\n",
    "        \n",
    "    def train(self, X, Y):\n",
    "        ## X : list of tokens (n,t)\n",
    "        ## Y : list of class (n,c)\n",
    "        ## return token\n",
    "        X_array = self.list2onehot(X)\n",
    "        Y_array = self.class2onehot(Y)\n",
    "        self.sklearn_model.fit(X_array, Y_array)\n",
    "        feature_importance_list = list(zip([tup[0] for tup in sorted(list(self.enc.vocabulary_.items()), key=lambda x : x[1])], self.sklearn_model.feature_importances_.tolist()))\n",
    "        feature_importance_list = sorted(feature_importance_list, key = lambda x : -x[1])\n",
    "        return feature_importance_list\n",
    "    \n",
    "    def predict(self, X, type=\"str\"):\n",
    "        X_array = self.list2onehot(X, mode=\"test\")\n",
    "        #Y_array = self.class2onehot(Y, mode=\"test\")\n",
    "        pred = self.sklearn_model.predict(X_array)\n",
    "        return self.lenc.inverse_transform(pred) if type==\"str\" else pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_array = self.list2onehot(X, mode=\"test\")\n",
    "        #Y_array = self.class2onehot(Y, mode=\"test\")\n",
    "        pred = self.sklearn_model.predict_proba(X_array)\n",
    "        return np.array([r[:,1] for r in pred]).T\n",
    "    \n",
    "    def score(self, X, Y, k=5):\n",
    "        X_array = self.list2onehot(X, mode=\"test\")\n",
    "        Y_array = self.class2onehot(Y, mode=\"test\")\n",
    "        pred = self.predict(X, type=\"num\")\n",
    "        \n",
    "        acc = self.sklearn_model.score(X_array, Y_array)\n",
    "        precision = precision_score(Y_array, pred, average='micro')\n",
    "        recall = recall_score(Y_array, pred, average='micro')\n",
    "        pr_metrix = precision_recall_fscore_support(Y_array, pred, average=None)\n",
    "        \n",
    "        pred_proba = self.predict_proba(X)\n",
    "        topk_class = np.argsort(pred_proba, axis=1)[:,-k:]\n",
    "        topk_pred = np.zeros_like(pred)\n",
    "        for i in range(pred.shape[0]):\n",
    "            topk_pred[i, topk_class[i]] = 1\n",
    "        \n",
    "        topk = recall_score(Y_array, topk_pred, average='micro')\n",
    "        return acc, recall, precision, pr_metrix, topk\n",
    "    \n",
    "    def get_depth(self):\n",
    "        return self.sklearn_model.get_depth()\n",
    "    def get_leaves(self):\n",
    "        return self.sklearn_model.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3934ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.decision_tree import TokenDecisionTreeModel\n",
    "#     model\n",
    "#     self.enc = TfidfVectorizer\n",
    "#     self.lenc = CountVectorizer\n",
    "\n",
    "def score_tokenize_random_forest(df , token_col_name, class_col_name, print_mes = False, criterion = 'gini',max_depth =None, min_samples_split = 2, test_size = 0.3, random_state = 42, threshold = 0.05):\n",
    "    \n",
    "    train, test = train_test_split(df, test_size = test_size, random_state= random_state)\n",
    "    \n",
    "    train_token_list = list(train[token_col_name])\n",
    "    train_class_list = list(train[class_col_name])\n",
    "    test_token_list = list(test[token_col_name])\n",
    "    test_class_list = list(test[class_col_name])\n",
    "\n",
    "    print(f'training dataset size: {len(train_token_list)}')\n",
    "    print(f'testing dataset size: {len(test_token_list)}')\n",
    "    model = RandomForestModel(dt_args={\"random_state\":42, \"criterion\": criterion})\n",
    "    \n",
    "    model.list2onehot(train_token_list)\n",
    "    model.class2onehot(train_class_list)\n",
    "    \n",
    "    print(f'x_columns_size: {len(model.enc.vocabulary_)}')\n",
    "    print(f'y_columns_size: {len(model.lenc.vocabulary_)}')\n",
    "    \n",
    "    # print(model.lenc.vocabulary_)\n",
    "    \n",
    "    feature_importance_list = model.train(train_token_list, train_class_list)\n",
    "    print(f'feature_importance_list: {len(feature_importance_list)}')\n",
    "    print(f'feature_importance_list: {feature_importance_list[:30]}')\n",
    "    \n",
    "    # predict\n",
    "    predict_return_matrix_num = model.predict(test_token_list ,type='num')\n",
    "    predict_return_matrix = model.predict(test_token_list)\n",
    "\n",
    "    # score\n",
    "    acc, recall, precision, pr_metrix, topk = model.score(test_token_list, test_class_list)\n",
    "    score_df = pd.DataFrame([{'acc': acc, 'recall': recall, 'precision': precision, 'topk':topk}])\n",
    "\n",
    "    # 印出一些東東\n",
    "    if print_mes:   \n",
    "        acutual_average = model.class2onehot(df[class_col_name]).sum(axis=1).mean()\n",
    "        acutual_distribution = model.class2onehot(df[class_col_name]).sum(axis=0)\n",
    "        print(f'原始資料： {class_col_name} 數目：{acutual_average}')\n",
    "        print(f'原始資料：每個 {class_col_name} 被使用之數目：{acutual_distribution}')\n",
    "        print(f'實際{class_col_name}數 {len(acutual_distribution)}')\n",
    "        \n",
    "        predict_average = predict_return_matrix_num.sum(axis=1).mean()\n",
    "        predict_distribution = predict_return_matrix_num.sum(axis=0)\n",
    "        print(f'預測中：平均 {class_col_name} 數目：{predict_average}')\n",
    "        print(f'預測中：每個 {class_col_name} 被預測之數目：{predict_distribution}')\n",
    "        print(f'預測{class_col_name}數 {len(predict_distribution)}')\n",
    "    \n",
    "    return model, score_df.T, pr_metrix, predict_return_matrix, predict_return_matrix_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cf8bbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size: 8000\n",
      "testing dataset size: 2000\n",
      "x_columns_size: 191905\n",
      "y_columns_size: 99\n",
      "feature_importance_list: 191905\n",
      "feature_importance_list: [('data', 0.0018458718693188548), ('I', 0.0018238094950946972), ('writing', 0.0017555846008218182), ('to', 0.001668959440200888), ('my', 0.0015536784470532345), ('of', 0.0015309651067946203), ('the', 0.0014750285447264744), ('and', 0.0014742433982254945), ('you', 0.001457665909843773), ('is', 0.001443787626257395), ('blockchain', 0.0013371252225262786), ('cryptocurrency', 0.0013260398459437697), ('code', 0.0013161633579005466), ('can', 0.0013128532498365073), ('a', 0.0013118642515050412), ('for', 0.0012984921427375126), ('in', 0.00128677004659046), ('Python', 0.0012800446826984604), ('design', 0.0012721296643625369), ('your', 0.001194747498620924), ('using', 0.0011901110591842517), ('on', 0.0011884319319384862), ('are', 0.0011827572364805175), ('that', 0.00117803478703971), ('use', 0.0011716010136919948), ('Data', 0.0011715088941716852), ('React', 0.0011658634339552277), ('crypto', 0.0011575459111961073), ('be', 0.0011330673342190888), ('was', 0.0010963296954506051)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/lalami/lalami_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始資料： reduced_tags 數目：2.6079\n",
      "原始資料：每個 reduced_tags 被使用之數目：[ 216  150  175  548    1  565  589  222  315   68   60   58  114  179\n",
      "  164  179  468  143  516  285  159  987  373  201   11  488  131  231\n",
      "  298  130  195  131   89  180   43  100   73   53  380  156  336   85\n",
      "  211   84  441   95  123   68  614  559  111  258  677  398  520  122\n",
      "  109  114   55  240   88   40   95  143  124   88  683   66  226  432\n",
      " 1135  388  657   45  227  292  116  246  446  539  195  135  228  195\n",
      "  621  266  137   28  508  168  152  619   89   44  203  255  123  226\n",
      "  990  177]\n",
      "實際reduced_tags數 100\n",
      "預測中：平均 reduced_tags 數目：0.0555\n",
      "預測中：每個 reduced_tags 被預測之數目：[ 0  0  1  0 11  3  0  0  0  0  0  0  0  1  1  1  0  6  0  0 19 12  0  0\n",
      "  1  0  0  0  0  0  0  0  2  0  0  0  0  0  0  9  0  0  0  2  0  0  0  1\n",
      "  0  0  1  2  0  1  0  0  0  0  0  1  0  0  0  0  0 11  0  0  1  8  1  3\n",
      "  0  0  0  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n",
      "  1  8  0]\n",
      "預測reduced_tags數 99\n",
      "CPU times: user 3min 15s, sys: 7.2 s, total: 3min 22s\n",
      "Wall time: 3min 25s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>acc</th>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.016584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.783784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topk</th>\n",
       "      <td>0.475791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "acc        0.008000\n",
       "recall     0.016584\n",
       "precision  0.783784\n",
       "topk       0.475791"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model, score_df, pr_metrix, predict_return_matrix, predict_return_matrix_num \\\n",
    "            = score_tokenize_random_forest(df, TEXT_TOKEN_COL_NAME, CLASS_COL_NAME,\\\n",
    "                             test_size = TEST_SIZE, random_state = RANDOM_STATE, \\\n",
    "                             print_mes= True) \n",
    "with open(RF_SAVE_FILE_NAME, 'wb') as files:\n",
    "    pickle.dump(model.sklearn_model, files)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e73ed3",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d991a90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f59a4172210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07116d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea48f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast, AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2bd25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51c559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10994691",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c473d95",
   "metadata": {},
   "source": [
    "### 資料前處理相關變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56cfed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## Global Variables\n",
    "\n",
    "# BERT Embedding Tensor 儲存與載入的檔案路徑\n",
    "EMBEDDING_FILE_PATH = \"./data/embedding_tensor.pt\"\n",
    "\n",
    "# SUBSENTENCE_NUM: 設定每篇文章的子句 (Segment) 數量\n",
    "SUBSENTENCE_NUM = 10\n",
    "\n",
    "# CHUNK_SIZE: 設定每個子句 (Segment) 最長只能多少個字\n",
    "CHUNK_SIZE = 128\n",
    "\n",
    "# BERT Tokenizer\n",
    "TOKENIZER = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# BERT Model\n",
    "BERT = AutoModel.from_pretrained('bert-base-uncased')\n",
    "BERT = BERT.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343edea3",
   "metadata": {},
   "source": [
    "### chunk_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "290d8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sentence(text, chunk_size=CHUNK_SIZE):\n",
    "    sub_text_list = []\n",
    "    text_tokens = text.split(' ')\n",
    "    delimiter = ['.', '!', '?']\n",
    "    if len(text_tokens) <= CHUNK_SIZE:\n",
    "        return [text]\n",
    "    \n",
    "    else:\n",
    "        while len(text_tokens) > chunk_size:\n",
    "            find_end_token = False\n",
    "       \n",
    "            for i in reversed(range(chunk_size)):\n",
    "                for j in delimiter:\n",
    "                    if j in text_tokens[i]:\n",
    "                        find_end_token = True\n",
    "                        end_token_idx = i\n",
    "                        break\n",
    "                if find_end_token or (i == 0 and not find_end_token):\n",
    "                    if i == 0 and not find_end_token:\n",
    "                        end_token_idx = chunk_size\n",
    "                    sub_text = ' '.join(text_tokens[:end_token_idx+1])\n",
    "                    sub_text_list.append(sub_text)\n",
    "                    text_tokens = text_tokens[end_token_idx+1:]\n",
    "                    break\n",
    "               \n",
    "                    \n",
    "        sub_text_list.append(' '.join(text_tokens))\n",
    "    return sub_text_list\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaf53dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"text\"] = df[\"text_token\"].apply(lambda x: len(x))\n",
    "# df[\"text_token_len\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3504148a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.51 s, sys: 536 ms, total: 4.05 s\n",
      "Wall time: 4.16 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "      <th>text_length</th>\n",
       "      <th>reduced_tags</th>\n",
       "      <th>text_token</th>\n",
       "      <th>text_token_stop</th>\n",
       "      <th>text_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 4 Purposes of Dreams</td>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "      <td>https://medium.com/science-for-real/the-4-purp...</td>\n",
       "      <td>['Eshan Samaranayake']</td>\n",
       "      <td>2020-12-21 16:05:19.524000+00:00</td>\n",
       "      <td>['Health', 'Neuroscience', 'Mental Health', 'P...</td>\n",
       "      <td>146</td>\n",
       "      <td>[Health, Mental Health, Psychology, Science]</td>\n",
       "      <td>[Passionate, about, the, synergy, between, sci...</td>\n",
       "      <td>[Passionate, synergy, science, technology, pro...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Surviving a Rod Through the Head</td>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "      <td>https://medium.com/live-your-life-on-purpose/s...</td>\n",
       "      <td>['Rishav Sinha']</td>\n",
       "      <td>2020-02-26 00:01:01.576000+00:00</td>\n",
       "      <td>['Brain', 'Health', 'Development', 'Psychology...</td>\n",
       "      <td>2326</td>\n",
       "      <td>[Health, Psychology, Science]</td>\n",
       "      <td>[You’ve, heard, of, him,, haven’t, you?, Phine...</td>\n",
       "      <td>[You’ve, heard, him,, haven’t, you?, Phineas, ...</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Quickly Build Trust, Tell Your Origin Story</td>\n",
       "      <td>Photo credit: Leo Leung People want to know wh...</td>\n",
       "      <td>https://medium.com/the-mission/want-trust-shar...</td>\n",
       "      <td>['Andy Raskin']</td>\n",
       "      <td>2016-07-06 19:45:00.648000+00:00</td>\n",
       "      <td>['Entrepreneurship', 'Personal Development', '...</td>\n",
       "      <td>4982</td>\n",
       "      <td>[Entrepreneurship, Personal Development, Start...</td>\n",
       "      <td>[Photo, credit:, Leo, Leung, People, want, to,...</td>\n",
       "      <td>[Photo, credit:, Leo, Leung, People, want, kno...</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facing Three Fundamental Coronavirus Fears</td>\n",
       "      <td>1. Since immunity to the novel coronavirus may...</td>\n",
       "      <td>https://medium.com/microbial-instincts/facing-...</td>\n",
       "      <td>['Bo Stapler']</td>\n",
       "      <td>2020-08-11 19:37:46.296000+00:00</td>\n",
       "      <td>['Health', 'Science', 'Wellness', 'Coronavirus...</td>\n",
       "      <td>3290</td>\n",
       "      <td>[Health, Science, Coronavirus, Covid 19]</td>\n",
       "      <td>[1., Since, immunity, to, the, novel, coronavi...</td>\n",
       "      <td>[1., Since, immunity, novel, coronavirus, may,...</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This 10-Minute Routine Will Increase Your Clar...</td>\n",
       "      <td>“Your subconscious mind works continuously, wh...</td>\n",
       "      <td>https://medium.com/the-mission/this-10-minute-...</td>\n",
       "      <td>['Benjamin Hardy']</td>\n",
       "      <td>2020-09-21 20:10:38.871000+00:00</td>\n",
       "      <td>['Productivity', 'Creativity', 'Motivation', '...</td>\n",
       "      <td>4446</td>\n",
       "      <td>[Productivity, Creativity, Motivation, Startup...</td>\n",
       "      <td>[“Your, subconscious, mind, works, continuousl...</td>\n",
       "      <td>[“Your, subconscious, mind, works, continuousl...</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                           The 4 Purposes of Dreams   \n",
       "1                   Surviving a Rod Through the Head   \n",
       "2     To Quickly Build Trust, Tell Your Origin Story   \n",
       "3         Facing Three Fundamental Coronavirus Fears   \n",
       "4  This 10-Minute Routine Will Increase Your Clar...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Passionate about the synergy between science a...   \n",
       "1  You’ve heard of him, haven’t you? Phineas Gage...   \n",
       "2  Photo credit: Leo Leung People want to know wh...   \n",
       "3  1. Since immunity to the novel coronavirus may...   \n",
       "4  “Your subconscious mind works continuously, wh...   \n",
       "\n",
       "                                                 url                 authors  \\\n",
       "0  https://medium.com/science-for-real/the-4-purp...  ['Eshan Samaranayake']   \n",
       "1  https://medium.com/live-your-life-on-purpose/s...        ['Rishav Sinha']   \n",
       "2  https://medium.com/the-mission/want-trust-shar...         ['Andy Raskin']   \n",
       "3  https://medium.com/microbial-instincts/facing-...          ['Bo Stapler']   \n",
       "4  https://medium.com/the-mission/this-10-minute-...      ['Benjamin Hardy']   \n",
       "\n",
       "                          timestamp  \\\n",
       "0  2020-12-21 16:05:19.524000+00:00   \n",
       "1  2020-02-26 00:01:01.576000+00:00   \n",
       "2  2016-07-06 19:45:00.648000+00:00   \n",
       "3  2020-08-11 19:37:46.296000+00:00   \n",
       "4  2020-09-21 20:10:38.871000+00:00   \n",
       "\n",
       "                                                tags  text_length  \\\n",
       "0  ['Health', 'Neuroscience', 'Mental Health', 'P...          146   \n",
       "1  ['Brain', 'Health', 'Development', 'Psychology...         2326   \n",
       "2  ['Entrepreneurship', 'Personal Development', '...         4982   \n",
       "3  ['Health', 'Science', 'Wellness', 'Coronavirus...         3290   \n",
       "4  ['Productivity', 'Creativity', 'Motivation', '...         4446   \n",
       "\n",
       "                                        reduced_tags  \\\n",
       "0       [Health, Mental Health, Psychology, Science]   \n",
       "1                      [Health, Psychology, Science]   \n",
       "2  [Entrepreneurship, Personal Development, Start...   \n",
       "3           [Health, Science, Coronavirus, Covid 19]   \n",
       "4  [Productivity, Creativity, Motivation, Startup...   \n",
       "\n",
       "                                          text_token  \\\n",
       "0  [Passionate, about, the, synergy, between, sci...   \n",
       "1  [You’ve, heard, of, him,, haven’t, you?, Phine...   \n",
       "2  [Photo, credit:, Leo, Leung, People, want, to,...   \n",
       "3  [1., Since, immunity, to, the, novel, coronavi...   \n",
       "4  [“Your, subconscious, mind, works, continuousl...   \n",
       "\n",
       "                                     text_token_stop  text_after  \n",
       "0  [Passionate, synergy, science, technology, pro...          12  \n",
       "1  [You’ve, heard, him,, haven’t, you?, Phineas, ...         234  \n",
       "2  [Photo, credit:, Leo, Leung, People, want, kno...         535  \n",
       "3  [1., Since, immunity, novel, coronavirus, may,...         297  \n",
       "4  [“Your, subconscious, mind, works, continuousl...         426  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_pickle(DF_FILE_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec828540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.25 s, sys: 49.3 ms, total: 2.3 s\n",
      "Wall time: 2.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 切割子字串\n",
    "# df=df[:1000]\n",
    "df[\"text_split\"] = df[\"text\"].apply(chunk_sentence, chunk_size=CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ecd0ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "df[\"text_split_len\"] = df[\"text_split\"].apply(lambda x: len(x))\n",
    "print(max(df[\"text_split_len\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15af6e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/lalami/lalami_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/adam/lalami/lalami_env/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "text_lists = df[TEXT_COL_NAME]\n",
    "        \n",
    "# 確保所有文章的句數一致 (所有文章的 Segment 數量都要等於 SUBSENTENCE_NUM)\n",
    "for i in range(len(text_lists)):\n",
    "    text_len = len(text_lists[i])\n",
    "    if text_len < SUBSENTENCE_NUM:\n",
    "        tmp = [\"\"] * (SUBSENTENCE_NUM-text_len)\n",
    "        text_lists[i] += tmp\n",
    "    else:\n",
    "        text_lists[i] = text_lists[i][:SUBSENTENCE_NUM]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4385670",
   "metadata": {},
   "source": [
    "### 把 BERT Embedding 先算好存起來 (embedding_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7dc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_tensor(text_lists):\n",
    "    EMBEDDING_BATCH_FILE_PATH = \"./data/embedding_tensor_\"\n",
    "    embedding_tensor = torch.ones((len(text_lists), SUBSENTENCE_NUM, 768))\n",
    "#     print(embedding_tensor)\n",
    "    count = 0\n",
    "    for i in range(len(text_lists)):\n",
    "        text_list = text_lists[i]\n",
    "        embedding = []\n",
    "        for text in text_list:\n",
    "            bert_input = TOKENIZER(text, padding='max_length', max_length=CHUNK_SIZE,  # 128\n",
    "                            truncation=True, return_tensors=\"pt\")\n",
    "            attention_mask = bert_input['attention_mask'].to(device)\n",
    "            input_id = bert_input['input_ids'].squeeze(1).to(device)\n",
    "            final_inputs = {'input_ids': input_id, 'attention_mask': attention_mask}\n",
    "            outputs = BERT(**final_inputs)\n",
    "            pooler_output = outputs.pooler_output.reshape(768)  # torch.Size([1, 768]) -> torch.Size([768])\n",
    "            embedding.append(pooler_output)\n",
    "        embedding = torch.stack(embedding).detach().cpu()  # torch.Size([n, 768])\n",
    "        count += 1\n",
    "        print(f\"Finish: {count}/{len(text_lists)}\")\n",
    "        print(embedding)\n",
    "        embedding_tensor[i] = embedding\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        if count % 10000 == 0:\n",
    "            torch.save(embedding_tensor, EMBEDDING_BATCH_FILE_PATH+str(count)+\".pt\")\n",
    "    return embedding_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef6f93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 如果要計算新的 Embedding 的話，就把這個 cell 的註解拿掉\n",
    "\n",
    "# 計算每篇文章各自的 BERT Embedding\n",
    "embedding_tensor = get_embedding_tensor(text_lists)\n",
    "\n",
    "# 儲存 BERT Embedding Tensor\n",
    "torch.save(embedding_tensor, EMBEDDING_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c56a2cb",
   "metadata": {},
   "source": [
    "### 切割訓練與測試資料 (train_df & test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7448d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切割「訓練」與「測試」資料 (train_df, test_df)\n",
    "train_df, test_df = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6378642",
   "metadata": {},
   "source": [
    "## 定義相關函數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a231de3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 定義「計算 Positive Class Weight」的函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "646fa024",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_pos_weights(df, label, mlb):\n",
    "    # 記住有哪些可能的類別\n",
    "    classes_ = mlb.classes_\n",
    "    # 計算每個類別的「正樣本數量」，儲存成 class_pos_count 列表\n",
    "    class_pos_count = np.zeros(len(classes_))\n",
    "    for i in range(len(classes_)):\n",
    "        pos_count = 0\n",
    "        for label_list in df[label]:\n",
    "            if classes_[i] in label_list:\n",
    "                pos_count += 1\n",
    "        class_pos_count[i] = pos_count\n",
    "    # 計算每個類別的「負樣本數量」，儲存成 class_neg_count 列表\n",
    "    class_neg_count = [len(df) - pos_count for pos_count in class_pos_count]\n",
    "    # 計算「正樣本權重」\n",
    "    pos_weight = sum(class_neg_count) / sum(class_pos_count)\n",
    "    class_pos_weight = np.array([math.sqrt(math.sqrt(pos_weight))] * len(classes_))\n",
    "#     class_pos_weight = np.ones_like(class_pos_count)\n",
    "#     for cdx, (pos_count, neg_count) in enumerate(zip(class_pos_count, class_neg_count)):\n",
    "#         class_pos_weight[cdx] = math.sqrt(math.sqrt(neg_count/(pos_count+1e-5)))\n",
    "    \n",
    "    return torch.as_tensor(class_pos_weight, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407529e8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###  定義「計算評估指標」的函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e40f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function: 計算 Accuracy, Precision, Recall, Top-k\n",
    "\n",
    "def _ACCscore(y_true, pred):\n",
    "    accuracy_l = [ans.all() for ans in (pred == y_true)]\n",
    "    accuracy = np.array(accuracy_l).mean()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def _PRscore(y_true, pred):\n",
    "    hit_matrix = np.zeros_like(pred)\n",
    "    hit_matrix[np.where((pred == y_true) & (y_true > 0))] = 1\n",
    "    tp = hit_matrix.sum(axis=1)\n",
    "    pred_sum = pred.sum(axis=1)\n",
    "    true_sum = y_true.sum(axis=1)\n",
    "    precision_l = []\n",
    "    recall_l = []\n",
    "    for ix in range(tp.shape[0]):\n",
    "        precision_score = (1.0 if true_sum[ix] == 0 else 0.0) if pred_sum[ix] == 0 else tp[ix]/pred_sum[ix]\n",
    "        recall_score = (1.0 if pred_sum[ix] == 0 else 0.0) if true_sum[ix] == 0 else tp[ix]/true_sum[ix]\n",
    "        precision_l.append(precision_score)\n",
    "        recall_l.append(recall_score)\n",
    "    precision = np.array(precision_l).mean()\n",
    "    recall = np.array(recall_l).mean()\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def _TopKscore(y_true, pred, output, k):\n",
    "    topk_class = np.argsort(output, axis=1)[:,-k:]\n",
    "    topk_pred = torch.zeros_like(pred)\n",
    "    for i in range(pred.shape[0]):\n",
    "        if pred[i, :].sum() > 0:\n",
    "            topk_pred[i, topk_class[i]] = 1\n",
    "    topk_precision, topk_recall = _PRscore(y_true, topk_pred)\n",
    "    return topk_precision, topk_recall\n",
    "\n",
    "# Function: 一次同時計算所有評估指標\n",
    "\n",
    "def score(y_true, pred, output, k=5):\n",
    "    accuracy = _ACCscore(y_true, pred)\n",
    "    precision, recall = _PRscore(y_true, pred)\n",
    "    _, topk_recall = _TopKscore(y_true, pred, output, k)\n",
    "    return accuracy, precision, recall, topk_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87eec74",
   "metadata": {},
   "source": [
    "## 定義分類任務相關變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目標欄位名稱 (Target Label)\n",
    "LABEL = \"tags\"\n",
    "\n",
    "# Multi-hot Lable Vector\n",
    "MLB = MultiLabelBinarizer()\n",
    "MLB.fit(df[LABEL])  # array([[1, 1, 0...], [0, 1, 0...] ...])\n",
    "\n",
    "# Class (Pos) Weight\n",
    "POS_WEIGHT = calculate_pos_weights(df=train_df, label=LABEL, mlb=MLB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"<< 目標變數與權重對應 >>\\n\")\n",
    "print(f\"<< 目標變數共有 {len(MLB.classes_)} 類 >>\\n\")\n",
    "\n",
    "for c, w in zip(MLB.classes_, POS_WEIGHT):\n",
    "    print(f\"{c}: {round(float(w), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59771db5",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b7780",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 定義 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7591c6ba",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, embedding):\n",
    "        self.embeddings = embedding\n",
    "        self.labels = MLB.transform(df[LABEL])  # [[0,0,1], [0,1,1]...]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_embeddings(self, idx):\n",
    "        # Fetch a batch of embeddings\n",
    "        return self.embeddings[idx]\n",
    "    \n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return self.labels[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_embeddings = self.get_batch_embeddings(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "        return batch_embeddings, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210fd69",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 定義 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "587fa438",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BertLSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):  # LABEL_NUM 表示輸出類別數量\n",
    "        super(BertLSTMClassifier, self).__init__()\n",
    "        self.linear_1 = nn.Linear(768, 512)  # Layer to convert BERT embedding into our task's vector space\n",
    "        self.lstm = nn.LSTM(512, 128, batch_first=True, bidirectional=True)  # input_size=512, hidden_size=128*2\n",
    "        self.linear_2 = nn.Linear(256, LABEL_NUM)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        linear_1_output = self.linear_1(embeddings)  # Input: (batch_size, sequence_length, 768)\n",
    "        lstm_output, (ht, ct) = self.lstm(linear_1_output)\n",
    "        ht = torch.cat((ht[0], ht[1]), 1)  # 把 BiLSTM 的兩層結果接起來\n",
    "        ht = ht.unsqueeze(0)  # 添加第0維\n",
    "        linear_2_output = self.linear_2(ht[-1])\n",
    "        return linear_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "698a005a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ##### 沒有 FC 降維 #####\n",
    "\n",
    "# class BertLSTMClassifier(nn.Module):\n",
    "\n",
    "#     def __init__(self):  # LABEL_NUM 表示輸出類別數量\n",
    "#         super(BertLSTMClassifier, self).__init__()\n",
    "# #         self.linear_1 = nn.Linear(768, 512)  # Layer to convert BERT embedding into our task's vector space\n",
    "#         self.lstm = nn.LSTM(768, 128, batch_first=True, bidirectional=True)  # input_size=768, hidden_size=128\n",
    "#         self.linear = nn.Linear(256, LABEL_NUM)\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "# #         linear_1_output = self.linear_1(embeddings)  # Input: (batch_size, sequence_length, 768)\n",
    "#         lstm_output, (ht, ct) = self.lstm(embeddings)\n",
    "#         ht = torch.cat((ht[0], ht[1]), 1)  # 把 BiLSTM 的兩層結果接起來\n",
    "#         ht = ht.unsqueeze(0)  # 添加第0維\n",
    "#         linear_output = self.linear(ht[-1])\n",
    "#         return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b1a76a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ##### 兩層 BiLSTM #####\n",
    "\n",
    "# class BertLSTMClassifier(nn.Module):\n",
    "\n",
    "#     def __init__(self):  # LABEL_NUM 表示輸出類別數量\n",
    "#         super(BertLSTMClassifier, self).__init__()\n",
    "#         self.linear_1 = nn.Linear(768, 512)  # Layer to convert BERT embedding into our task's vector space\n",
    "#         self.lstm_1 = nn.LSTM(512, 128, batch_first=True, bidirectional=True)# input_size=512, hidden_size=128*2\n",
    "#         self.lstm_2 = nn.LSTM(256, 64, batch_first=True, bidirectional=True)# input_size=512, hidden_size=128*2\n",
    "#         self.linear_2 = nn.Linear(128, LABEL_NUM)\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "#         linear_1_output = self.linear_1(embeddings)  # Input: (batch_size, sequence_length, 768)\n",
    "#         lstm_1_output, (ht, ct) = self.lstm_1(linear_1_output)\n",
    "#         lstm_2_output, (ht, ct) = self.lstm_2(lstm_1_output)\n",
    "#         ht = torch.cat((ht[0], ht[1]), 1)  # 把 BiLSTM 的兩層結果接起來\n",
    "#         ht = ht.unsqueeze(0)  # 添加第0維\n",
    "#         linear_2_output = self.linear_2(ht[-1])\n",
    "#         return linear_2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0bc2a",
   "metadata": {},
   "source": [
    "### 定義訓練過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e033ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batch_size, patience):\n",
    "    \n",
    "    # 記錄 train 訓練過程\n",
    "    train_loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    train_precision_list = []\n",
    "    train_recall_list = []\n",
    "    train_topk_recall_list = []\n",
    "    \n",
    "    # 記錄 validation 訓練過程\n",
    "    val_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    val_precision_list = []\n",
    "    val_recall_list = []\n",
    "    val_topk_recall_list = []\n",
    "    \n",
    "    # Training and Test Dataset\n",
    "    train = Dataset(df=train_df, embedding=train_embedding)\n",
    "    val = Dataset(df=test_df, embedding=test_embedding)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "\n",
    "    # Loss Function\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=POS_WEIGHT)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # 把模型放到 GPU 上跑\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    # 將模型設定成「訓練」狀態（Dropout等Layer才會被觸發）\n",
    "    model.train()\n",
    "\n",
    "    # Early Stopping\n",
    "    trigger_times = 0\n",
    "    last_loss = 1e4\n",
    "    \n",
    "    # 訓練階段\n",
    "    for epoch_num in range(EPOCHS):\n",
    "        \n",
    "        # 訓練並儲存訓練結果\n",
    "        loss_list = []\n",
    "        accuracy_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        topk_recall_list = []\n",
    "\n",
    "        for train_embeddings, train_labels in tqdm(train_dataloader):\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_embeddings = train_embeddings.to(device)\n",
    "            train_labels = train_labels.to(device)\n",
    "\n",
    "            output = model(train_embeddings)\n",
    "\n",
    "            batch_loss = criterion(output, train_labels.float())\n",
    "            \n",
    "            # Back propagation\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            output = output.cpu().detach()\n",
    "            pred = (output > 0).cpu()\n",
    "            y_true = (train_labels > 0).cpu()\n",
    "        \n",
    "            # 計算 batch 表現並存起來\n",
    "            loss = batch_loss.item()\n",
    "            accuracy, precision, recall, topk_recall = score(y_true, pred, output, k=5)\n",
    "            \n",
    "            loss_list.append(loss)\n",
    "            accuracy_list.append(accuracy)\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            topk_recall_list.append(topk_recall)\n",
    "        \n",
    "        # 計算 EPOCH 表現並存起來\n",
    "        train_loss = np.array(loss_list).mean()\n",
    "        train_accuracy = np.array(accuracy_list).mean()\n",
    "        train_precision = np.array(precision_list).mean()\n",
    "        train_recall = np.array(recall_list).mean()\n",
    "        train_topk_recall = np.array(topk_recall_list).mean()\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        train_precision_list.append(train_precision)\n",
    "        train_recall_list.append(train_recall)\n",
    "        train_topk_recall_list.append(train_topk_recall)\n",
    "\n",
    "        # ===============================================================================\n",
    "        \n",
    "        # 驗證並儲存驗證結果\n",
    "        loss_list = []\n",
    "        accuracy_list = []\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        topk_recall_list = []\n",
    "        \n",
    "        # 驗證當下的模型表現\n",
    "        with torch.no_grad():\n",
    "            for val_embeddings, val_labels in val_dataloader:\n",
    "\n",
    "                val_embeddings = val_embeddings.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "\n",
    "                output = model(val_embeddings)\n",
    "                \n",
    "                batch_loss = criterion(output, val_labels.float())\n",
    "                \n",
    "                output = output.cpu()\n",
    "                pred = (output > 0).cpu()\n",
    "                y_true = (val_labels > 0).cpu()\n",
    "                \n",
    "                # 計算 batch 表現並存起來\n",
    "                loss = batch_loss.item()\n",
    "                accuracy, precision, recall, topk_recall = score(y_true, pred, output, k=5)\n",
    "                \n",
    "                loss_list.append(loss)\n",
    "                accuracy_list.append(accuracy)\n",
    "                precision_list.append(precision)\n",
    "                recall_list.append(recall)\n",
    "                topk_recall_list.append(topk_recall)\n",
    "            \n",
    "            # 計算 EPOCH 表現並存起來\n",
    "            val_loss = np.array(loss_list).mean()\n",
    "            val_accuracy = np.array(accuracy_list).mean()\n",
    "            val_precision = np.array(precision_list).mean()\n",
    "            val_recall = np.array(recall_list).mean()\n",
    "            val_topk_recall = np.array(topk_recall_list).mean()\n",
    "            \n",
    "            val_loss_list.append(val_loss)\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            val_precision_list.append(val_precision)\n",
    "            val_recall_list.append(val_recall)\n",
    "            val_topk_recall_list.append(val_topk_recall)\n",
    "        \n",
    "        # 印出每一個 Epoch 的結果\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {train_loss: .3f} \\\n",
    "            | Train Accuracy: {train_accuracy: .3f} \\\n",
    "            | Val Loss: {val_loss: .3f} \\\n",
    "            | Val Accuracy: {val_accuracy: .3f}')\n",
    "        \n",
    "        # Early Stopping\n",
    "        if val_loss >= last_loss:\n",
    "            trigger_times += 1\n",
    "            print('Trigger times:', trigger_times)\n",
    "\n",
    "            if trigger_times > patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                break\n",
    "        else:\n",
    "            print('Trigger times: 0')\n",
    "            trigger_times = 0\n",
    "            last_loss = val_loss\n",
    "            torch.save(model, NN_SAVE_FILE_NAME)\n",
    "        \n",
    "    return model, train_loss_list, train_precision_list, train_recall_list, \\\n",
    "                            val_loss_list, val_precision_list, val_recall_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62dc5cb",
   "metadata": {},
   "source": [
    "### 定義分類模型相關變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce35555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練次數\n",
    "EPOCHS = 150\n",
    "\n",
    "# 學習率\n",
    "LR = 1e-4\n",
    "\n",
    "# 輸出類別數量\n",
    "LABEL_NUM = len(MLB.classes_)\n",
    "\n",
    "# 最終模型\n",
    "model = BertLSTMClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e86c7",
   "metadata": {},
   "source": [
    "### 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd691ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_loss_list, train_precision_list, train_recall_list, \\\n",
    "            val_loss_list, val_precision_list, val_recall_list = train(model=model, batch_size=8, patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5bc836",
   "metadata": {},
   "source": [
    "### 視覺化訓練結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601154ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 Precision 結果\n",
    "sns.set()\n",
    "plt.plot(range(1, len(train_precision_list)+1), train_precision_list, label=\"train_precision\")\n",
    "plt.plot(range(1, len(val_precision_list)+1), val_precision_list, label=\"val_precision\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 Recall 結果\n",
    "sns.set()\n",
    "plt.plot(range(1, len(train_recall_list)+1), train_recall_list, label=\"train_recall\")\n",
    "plt.plot(range(1, len(val_recall_list)+1), val_recall_list, label=\"val_recall\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 Loss 結果\n",
    "plt.plot(range(1, len(train_loss_list)+1), train_loss_list, label=\"train_loss\")\n",
    "plt.plot(range(1, len(val_loss_list)+1), val_loss_list, label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea1f1f",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586cb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(NN_SAVE_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ba1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    test = Dataset(df=test_df, embedding=test_embedding)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    topk_recall_list = []\n",
    "    \n",
    "    y_pred_list = []\n",
    "    y_true_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_embeddings, test_labels in test_dataloader:\n",
    "            \n",
    "            test_embeddings = test_embeddings.to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "            \n",
    "            output = model(test_embeddings).cpu()\n",
    "\n",
    "            pred = (output > 0).cpu()\n",
    "            y_true = (test_labels > 0).cpu()\n",
    "            \n",
    "            # 計算評估指標\n",
    "            accuracy, precision, recall, topk_recall = score(y_true, pred, output, k=5)\n",
    "            print(f\"Accuracy: {accuracy}\")\n",
    "            print(f\"Precision: {precision}\")\n",
    "            print(f\"Recall: {recall}\")\n",
    "            print(f\"Top-5 Recall: {topk_recall}\")\n",
    "            accuracy_list.append(accuracy)\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            topk_recall_list.append(topk_recall)\n",
    "\n",
    "            # 印出猜測的結果對應\n",
    "            y_pred = list(MLB.classes_[pred[0]])\n",
    "            y_true = list(MLB.classes_[y_true[0]])\n",
    "            y_pred_list.append(y_pred)\n",
    "            y_true_list.append(y_true)\n",
    "            # if not MLB.classes_[y_true[0]].any():\n",
    "            print(f\"預測: {y_pred}\")\n",
    "            print(f\"正解: {y_true}\")\n",
    "            print(\"=\"*30)\n",
    "\n",
    "    accuracy_res = np.array(accuracy_list).mean()\n",
    "    precision_res = np.array(precision_list).mean()\n",
    "    recall_res = np.array(recall_list).mean()\n",
    "    topk_recall_res = np.array(topk_recall_list).mean()\n",
    "    \n",
    "    return accuracy_res, precision_res, recall_res, topk_recall_res, y_pred_list, y_true_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22569af",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_res, precision_res, recall_res, topk_recall_res, y_pred_list, y_true_list = evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094bd14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Data 結果:\")\n",
    "print(f\"Accuracy: {accuracy_res: .3f}\")\n",
    "print(f\"Precision: {precision_res: .3f}\")\n",
    "print(f\"Recall: {recall_res: .3f}\")\n",
    "print(f\"Top-5 Recall: {topk_recall_res: .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dffadf",
   "metadata": {},
   "source": [
    "### 各類別表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"目標變數數量: {len(list(MLB.classes_))} 類\")\n",
    "detail_res_dict = {c: {x: 0 for x in [\"TP\", \"TN\", \"FP\", \"FN\"]} for c in list(MLB.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7953430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算各類別的 TP, FP, TN, FN\n",
    "for pred_, true_ in zip(y_pred_list, y_true_list):\n",
    "    for guess in pred_:\n",
    "        if guess in true_:\n",
    "            detail_res_dict[guess][\"TP\"] += 1\n",
    "        else:\n",
    "            detail_res_dict[guess][\"FP\"] += 1\n",
    "    for label in true_:\n",
    "        if label not in pred_:\n",
    "            detail_res_dict[label][\"FN\"] += 1\n",
    "    others = set(MLB.classes_) - (set(pred_) | set(true_))\n",
    "    for each in others:\n",
    "        detail_res_dict[each][\"TN\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990fee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 Data Frame\n",
    "detail_res_df = pd.DataFrame(detail_res_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9cd594",
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_res_df[\"Recall\"] = round(detail_res_df[\"TP\"] / (detail_res_df[\"TP\"] + detail_res_df[\"FN\"]), 2)\n",
    "detail_res_df[\"Precision\"] = round(detail_res_df[\"TP\"] / (detail_res_df[\"TP\"] + detail_res_df[\"FP\"]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf165a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171e7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lalami_env",
   "language": "python",
   "name": "lalami_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
